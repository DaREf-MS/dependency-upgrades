{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367b9a7f48399d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T14:22:18.709735Z",
     "start_time": "2025-07-30T14:22:16.152399Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "from cliffs_delta import cliffs_delta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a5c46",
   "metadata": {},
   "source": [
    "### Configuration of Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac8d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –– Nature-ready styling\n",
    "FONT_SCALE = 1.10  # ← master knob\n",
    "_BASE_FONT = 12\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\", font_scale=FONT_SCALE)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    # 'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'Times'],\n",
    "    'mathtext.fontset': 'stix',\n",
    "    'axes.linewidth': 1.0,\n",
    "    'axes.edgecolor': '#000000',    \n",
    "    'axes.labelsize': _BASE_FONT,\n",
    "    'axes.titlesize': _BASE_FONT,\n",
    "    'xtick.major.size': 2,\n",
    "    'ytick.major.size': 2,\n",
    "    'xtick.labelsize': _BASE_FONT,\n",
    "    'ytick.labelsize': _BASE_FONT,\n",
    "    'legend.fontsize': _BASE_FONT,\n",
    "    'legend.title_fontsize': _BASE_FONT,\n",
    "    'figure.dpi': 300,\n",
    "    'axes.edgecolor': 'grey',\n",
    "    'axes.linewidth': 1\n",
    "})\n",
    "\n",
    "# Fancy boxplot with wider boxes\n",
    "colors = ['#66c2a5', '#fc8d62']\n",
    "\n",
    "def plot_boxplot(data, x, y, xlab, ylab, file, rotation=0, dpi=300):\n",
    "    fig = plt.figure(figsize=(3, 2), constrained_layout=True)\n",
    "    sns.boxplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        data=data,\n",
    "        # order=median_order,\n",
    "        hue=x,\n",
    "        legend=False,\n",
    "        palette=colors,\n",
    "        # palette='Set2', # Fancy color palette\n",
    "        patch_artist=True,\n",
    "        showfliers=False,\n",
    "        showcaps=False,\n",
    "        linewidth=0.8\n",
    "    )\n",
    "    plt.yscale('log') # This is the key step\n",
    "\n",
    "    # plt.title('Delay Time Distribution by Auto Merge Status')\n",
    "    plt.xlabel(xlab, fontsize=9)\n",
    "    plt.ylabel(ylab, fontsize=9)\n",
    "    plt.xticks(rotation=rotation, fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.grid(axis='y', linestyle='--')\n",
    "    plt.grid(False)\n",
    "    plt.savefig(file, bbox_inches='tight', dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6437f",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef92f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df = pd.read_csv('data/clean_prs.csv')\n",
    "\n",
    "all_upgrades_df = pd.concat(\n",
    "    (pd.read_csv(\"data/all_upgrades1.csv\"), pd.read_csv(\"data/all_upgrades2.csv\")))\n",
    "\n",
    "all_upgrades_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "all_upgrades_df['last_pr_merged_at'] = pd.to_datetime(\n",
    "    all_upgrades_df['last_pr_merged_at'], utc=True).dt.tz_convert(None)\n",
    "all_upgrades_df['last_pr_merged_at'] = pd.to_datetime(\n",
    "    all_upgrades_df['last_pr_merged_at'], errors='coerce')\n",
    "all_upgrades_df['pr_created_at'] = pd.to_datetime(\n",
    "    all_upgrades_df['pr_created_at'], utc=True).dt.tz_convert(None)\n",
    "all_upgrades_df['pr_created_at'] = pd.to_datetime(\n",
    "    all_upgrades_df['pr_created_at'], errors='coerce')\n",
    "all_upgrades_df[\"commit_label\"] = all_upgrades_df[\"commit_label\"].map(\n",
    "    lambda x: eval(x) if pd.notna(x) else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b6b546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.373888888888889"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_upgrades_df.loc[all_upgrades_df['label']=='Merged PR', 'delay_hours'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25630378",
   "metadata": {},
   "source": [
    "### PR dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231da1cdcc9fdccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repo_stats_table(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generate a LaTeX table with repository statistics from GitHub PR data.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing GitHub PR data with repository metadata\n",
    "        output_file (str): Path to output LaTeX file\n",
    "    \"\"\"\n",
    "    # Step 1: Remove duplicate repositories (keep first occurrence)\n",
    "    df_unique = df.sort_values(\"stargazer_count\", ascending=0).drop_duplicates(\n",
    "        subset=['repo'], keep='first').copy()\n",
    "\n",
    "    # Count security PRs per repository\n",
    "    security_prs = df.groupby('repo').size()\n",
    "    df_unique['dep_pr_count'] = df_unique['repo'].map(security_prs).fillna(0)\n",
    "\n",
    "    # Step 3: Prepare the statistics table\n",
    "    metrics = {\n",
    "        'Commits': 'repo_commit_count',\n",
    "        'Age': 'repo_age',\n",
    "        'Dep. PRs': 'dep_pr_count',\n",
    "        'PRs': 'pull_request_count',\n",
    "        'Stars': 'stargazer_count',\n",
    "        'Contributors': 'mentionable_users_count',\n",
    "        # 'Issues': 'issue_count',\n",
    "        # 'Users': 'mentionableUsersCount'\n",
    "    }\n",
    "\n",
    "    # Calculate statistics for each metric\n",
    "    stats_data = []\n",
    "    for metric_name, col_name in metrics.items():\n",
    "        values = df_unique[col_name]\n",
    "        stats_data.append({\n",
    "            'Metric': metric_name,\n",
    "            'Min.': np.min(values),\n",
    "            'Median': np.median(values),\n",
    "            'Mean': np.mean(values),\n",
    "            'Max.': np.max(values)\n",
    "        })\n",
    "\n",
    "    # Create stats DataFrame\n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    stats_df['Max.'] = stats_df['Max.'].apply(lambda x: f\"{x:,}\")\n",
    "    stats_df['Median'] = stats_df['Median'].apply(lambda x: f\"{x:,.2f}\")\n",
    "    stats_df['Mean'] = stats_df['Mean'].apply(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "    # Step 4: Generate LaTeX table\n",
    "    latex_table = stats_df.to_latex(\n",
    "        index=False,\n",
    "        column_format='lrrrr',\n",
    "        float_format=\"{:0.2f}\".format,\n",
    "        caption=f'Statistics of the {df['repo'].nunique():,} studied JavaScript projects.',\n",
    "        label='tab:paper3-repo_stats',\n",
    "        position='t!'\n",
    "    )\n",
    "    latex_table = latex_table.replace(\".00\", \"\")\n",
    "    # Save to file\n",
    "    # with open(output_file, 'w') as f:\n",
    "    #     f.write(latex_table)\n",
    "\n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9e43a669979b7054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[t!]\n",
      "\\caption{Statistics of the 697 studied JavaScript projects.}\n",
      "\\label{tab:paper3-repo_stats}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "Metric & Min. & Median & Mean & Max. \\\\\n",
      "\\midrule\n",
      "Commits & 41 & 901 & 2,243.35 & 72,123 \\\\\n",
      "Age & 336 & 2,613 & 2,737.24 & 5,910 \\\\\n",
      "Dep. PRs & 5 & 47 & 132.81 & 999 \\\\\n",
      "PRs & 14 & 373 & 806.99 & 17,558 \\\\\n",
      "Stars & 10 & 108 & 622.20 & 10,428 \\\\\n",
      "Contributors & 5 & 31 & 104.12 & 7,103 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_repo_stats_table(prs_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e636af",
   "metadata": {},
   "source": [
    "### Repository characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804a81c627c6b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[!hbtp]\n",
      "\\caption{The total count and percentgae of Dependabot security PRs acress different states}\n",
      "\\label{tab:dependabot_prs}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "\\textbf{Dependabot security PRs} & \\textbf{\\#} & \\textbf{\\%} \\\\\n",
      "\\midrule\n",
      "Merged & 55,414 & 59.86\\% \\\\\n",
      "Closed & 35,142 & 37.96\\% \\\\\n",
      "Open & 2,011 & 2.17\\% \\\\\n",
      "\\midrule\n",
      "\\textbf{Total} & \\textbf{92,567} & \\textbf{100.00\\%} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Group by state and count PRs\n",
    "state_counts = prs_df['state'].value_counts().reset_index()\n",
    "state_counts.columns = ['state', 'count']\n",
    "\n",
    "state_counts['state'] = state_counts['state'].str.capitalize()\n",
    "\n",
    "# Step 3: Calculate percentage\n",
    "total_dependabot = state_counts['count'].sum()\n",
    "state_counts['percentage'] = (\n",
    "    state_counts['count'] / total_dependabot * 100).round(2)\n",
    "state_counts['percentage'] = state_counts['percentage'].apply(\n",
    "    lambda x: f\"{x:,.2f}\\\\%\")\n",
    "\n",
    "state_counts['count'] = state_counts['count'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# Step 4: Add a total row\n",
    "total_row = pd.DataFrame({\n",
    "    'state': ['\\\\textbf{Total}'],\n",
    "    'count': ['\\\\textbf{' + f\"{total_dependabot:,}\" + '}'],\n",
    "    'percentage': ['\\\\textbf{100.00\\\\%}']\n",
    "})\n",
    "state_counts = pd.concat([state_counts, total_row], ignore_index=True)\n",
    "\n",
    "state_counts.rename(columns={\n",
    "    'state': '\\\\textbf{Dependabot security PRs}',\n",
    "    'count': '\\\\textbf{\\\\#}',\n",
    "    'percentage': '\\\\textbf{\\\\%}'\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 5: Generate LaTeX table\n",
    "latex_table = state_counts.to_latex(\n",
    "    index=False,\n",
    "    caption='The total count and percentgae of Dependabot security PRs acress different states',\n",
    "    label='tab:dependabot_prs',\n",
    "    column_format='lrr',\n",
    "    position=\"!hbtp\"\n",
    ")\n",
    "latex_table = latex_table.replace('\\\\textbf{Total}', '\\\\midrule\\n\\\\textbf{Total}')\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "14940cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of studied Dependabot PRs: 92567\n",
      "The total number of studied repositories: 697\n",
      "The percentage of studied PRs merged by Dependabot: 13.58%\n",
      "The percentage of studied PRs merged by others: 86.42%\n",
      "The percentage of studied PRs closed by Dependabot: 83.77%\n",
      "The percentage of studied PRs closed by others: 16.230000000000004%\n"
     ]
    }
   ],
   "source": [
    "num_merged_prs = len(prs_df[prs_df['state'] == 'MERGED'])\n",
    "num_prs_merged_by_dep = len(\n",
    "    prs_df[(prs_df['state'] == 'MERGED') & (prs_df['merged_by'] == 'dependabot')])\n",
    "pctg_prs_merged_by_dep = round(\n",
    "    100 * (num_prs_merged_by_dep / num_merged_prs), 2)\n",
    "pctg_prs_merged_by_human = 100 - pctg_prs_merged_by_dep\n",
    "\n",
    "num_closed_prs = len(prs_df[prs_df['state'] == 'CLOSED'])\n",
    "num_prs_closed_by_dep = len(\n",
    "    prs_df[(prs_df['state'] == 'CLOSED') & (prs_df['closed_by'] == 'dependabot')])\n",
    "pctg_prs_closed_by_dep = round(\n",
    "    100 * (num_prs_closed_by_dep / num_closed_prs), 2)\n",
    "pctg_prs_closed_by_human = 100 - pctg_prs_closed_by_dep\n",
    "\n",
    "\n",
    "print(f\"The total number of studied Dependabot PRs: {len(prs_df)}\")\n",
    "print(f\"The total number of studied repositories: {prs_df['repo'].nunique()}\")\n",
    "print(\n",
    "    f\"The percentage of studied PRs merged by Dependabot: {pctg_prs_merged_by_dep}%\")\n",
    "print(\n",
    "    f\"The percentage of studied PRs merged by others: {pctg_prs_merged_by_human}%\")\n",
    "print(\n",
    "    f\"The percentage of studied PRs closed by Dependabot: {pctg_prs_closed_by_dep}%\")\n",
    "print(\n",
    "    f\"The percentage of studied PRs closed by others: {pctg_prs_closed_by_human}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8322f9b9",
   "metadata": {},
   "source": [
    "### RQ1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee3f7a",
   "metadata": {},
   "source": [
    "#### Findings\n",
    " 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "577f0b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[!htbp]\n",
      "\\caption{The distribution of Dependabot dependency upgrades.}\n",
      "\\label{tab:paper3-label_dep_upgrade_distribution}\n",
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "\\textbf{Label} & \\textbf{\\#} & \\textbf{\\%} & \\textbf{$\\tilde{Upg} (days)$} \\\\\n",
      "\\midrule\n",
      "Merged PR & 51,713 & 82.62\\% & 0.2 \\\\\n",
      "Closed PR with external upgrade & 4,661 & 7.45\\% & 4.6 \\\\\n",
      "Superseding merged PR & 4,616 & 7.37\\% & 22.6 \\\\\n",
      "Superseding closed PR with external upgrade & 1,601 & 2.56\\% & 43.5 \\\\\n",
      "\\textbf{Total} & \\textbf{62,591} & \\textbf{100.00\\%} & - \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of each label\n",
    "counts = all_upgrades_df[\"label\"].value_counts()\n",
    "\n",
    "# Compute exact percentages\n",
    "percentages = counts / counts.sum() * 100\n",
    "\n",
    "# Round percentages for display\n",
    "rounded_percentages = percentages.round(2)\n",
    "\n",
    "# Adjust the largest percentage to ensure sum is exactly 100%\n",
    "diff = 100 - rounded_percentages.sum()\n",
    "if diff != 0:\n",
    "    # Add/subtract the difference from the largest percentage\n",
    "    largest_idx = rounded_percentages.idxmax()\n",
    "    rounded_percentages[largest_idx] += diff\n",
    "\n",
    "# Median delay per label\n",
    "median_delay = all_upgrades_df.groupby(\n",
    "    \"label\")[\"delay_time\"].median() / (60 * 24)\n",
    "\n",
    "# Format values\n",
    "result = pd.DataFrame({\n",
    "    \"Label\": counts.index,\n",
    "    \"\\\\#\": counts.apply(lambda x: f\"{x:,.0f}\"),\n",
    "    \"\\\\%\": rounded_percentages.apply(lambda x: f\"{x:.2f}\\\\%\"),\n",
    "    # median delay\n",
    "    r\"$\\tilde{Upg} (days)$\": median_delay.apply(lambda x: f\"{x:.1f}\").reindex(counts.index)\n",
    "})\n",
    "\n",
    "# Add total row\n",
    "total_count = counts.sum()\n",
    "result.loc[len(result)] = [\n",
    "    \"\\\\textbf{Total}\",\n",
    "    \"\\\\textbf{\" + f\"{total_count:,.0f}\" + \"}\",\n",
    "    \"\\\\textbf{100.00\\\\%}\",\n",
    "    \"-\"\n",
    "]\n",
    "\n",
    "result.columns = [f\"\\\\textbf{{{col}}}\" for col in result.columns]\n",
    "\n",
    "# Convert to LaTeX\n",
    "latex_table = result.to_latex(\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    caption=\"The distribution of Dependabot dependency upgrades.\",\n",
    "    label=\"tab:paper3-label_dep_upgrade_distribution\",\n",
    "    position=\"!htbp\",   # h=here, t=top, b=bottom, p=page\n",
    ")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9651c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall median upgrade time is 8.03 hours (0.33 days).\n",
      "Our defined categories show a an upgrade 80.40 times than merged PRs\n",
      "The median upgrade time of Merged PRs is 4.37 hours.\n",
      "The median upgrade time of non-Merged PRs is 351.65 hours (14.65 days).\n"
     ]
    }
   ],
   "source": [
    "median_upgrade_time = all_upgrades_df['delay_hours'].median()\n",
    "v1 = all_upgrades_df.loc[all_upgrades_df['label']\n",
    "                         == 'Merged PR', 'delay_hours'].median()\n",
    "v2 = all_upgrades_df.loc[all_upgrades_df['label']\n",
    "                         != 'Merged PR', 'delay_hours'].median()\n",
    "print(f\"The overall median upgrade time is {median_upgrade_time:.2f} hours ({(median_upgrade_time/24):.2f} days).\")\n",
    "print(f\"Our defined categories show a an upgrade {v2/v1:.2f} times than merged PRs\")\n",
    "print(f\"The median upgrade time of Merged PRs is {v1:.2f} hours.\")\n",
    "print(f\"The median upgrade time of non-Merged PRs is {v2:.2f} hours ({v2/24:.2f} days).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f43b0",
   "metadata": {},
   "source": [
    "#### Plot the overall upgrade time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "962778d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overall_upgrade_time(data: pd.DataFrame, log_scale: bool = True, filename=None):\n",
    "    labels = ['Merged PR', 'Other upgrades']\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 3.3))\n",
    "    \n",
    "    sns.boxplot(\n",
    "        data=data, x=\"label\", y=\"delay_hours\",\n",
    "        # labels=labels,\n",
    "        # notch=True,\n",
    "        showfliers=True,\n",
    "        showcaps=False, #\n",
    "        patch_artist=True,\n",
    "        hue='label',\n",
    "        palette=colors,\n",
    "        saturation=0.8, linewidth=1.2, ax=ax,\n",
    "        medianprops={\"linewidth\": 1, \"color\": \"#000000\"},\n",
    "    )\n",
    "        \n",
    "    ax.set_xlabel('Upgrade type')\n",
    "    ax.set_ylabel('Upgrade time')\n",
    "\n",
    "    if log_scale:\n",
    "        ax.set_yscale(\"symlog\")\n",
    "    ax.set_ylim(-0.1)\n",
    "    hours_yticks = [0, 1, 2, 8, 24 * 2, 24 * 14, 24 * 90, 24 * 360, 24 * 1440]\n",
    "    ax.set_yticks(hours_yticks)\n",
    "    hours_ylabels = []\n",
    "    for h in hours_yticks:\n",
    "        if h < 24:\n",
    "            hours_ylabels.append(f\"{h}h\")\n",
    "        elif h < 24 * 30:\n",
    "            hours_ylabels.append(f\"{h // 24}d\")\n",
    "        else:\n",
    "            hours_ylabels.append(f\"{h // (24 * 30)}mo\")\n",
    "    ax.set_yticklabels(hours_ylabels)\n",
    "    \n",
    "    # Annotate counts inside boxes\n",
    "    labels_unique = data['label'].unique()\n",
    "    # In Seaborn 0.12+, each box is the first patch for the category in order\n",
    "    for i, label in enumerate(labels_unique):\n",
    "        n = len(data[data['label'] == label])\n",
    "        box_patch = ax.patches[i]  # the first patch per box\n",
    "        bbox = box_patch.get_path().get_extents()  # get bounding box from PathPatch\n",
    "        y_top = bbox.y1\n",
    "        y_bottom = bbox.y0\n",
    "        y_text = y_bottom + (y_top - y_bottom) * 0.80  # slightly below top\n",
    "        ax.text(i, y_text, f\"{n:,}\", ha='center', va='top', fontsize=12, color='black')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    uniq = [(h, l) for i, (h, l) in enumerate(zip(handles, labels))\n",
    "            if l not in labels[:i]]\n",
    "\n",
    "    # Only attempt to create a legend if 'uniq' is not empty\n",
    "    if uniq:\n",
    "        ax.legend(*zip(*uniq), frameon=False, ncol=2, loc=\"upper right\")\n",
    "\n",
    "    # Replace underscores with spaces / nicer names\n",
    "    # ax.set_xticklabels([NAME_MAPPING.get(lbl.get_text(), lbl.get_text())\n",
    "                        # for lbl in ax.get_xticklabels()])\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax.set_ylim(top=40*(data['delay_hours'].max()//24)) \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    # out_fp = Path(out_fp)\n",
    "    # out_fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if filename:\n",
    "        fig.savefig(filename, bbox_inches='tight', pad_inches=0.01, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f5da821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delay in days\n",
    "all_upgrades_df['delay_hours'] = all_upgrades_df['delay_time'] / (60)\n",
    "\n",
    "# Masks\n",
    "merged_mask = all_upgrades_df['label'] == 'Merged PR'\n",
    "delay_mask = all_upgrades_df['delay_time'] > 0\n",
    "\n",
    "# Extract delays with labels\n",
    "merged_df = all_upgrades_df.loc[merged_mask & delay_mask, ['delay_hours']].copy()\n",
    "merged_df['label'] = 'Merged PRs'\n",
    "\n",
    "other_df = all_upgrades_df.loc[~merged_mask & delay_mask, ['delay_hours']].copy()\n",
    "other_df['label'] = 'Other upgrades'\n",
    "\n",
    "# Concatenate into a single long-format DataFrame\n",
    "delay_df = pd.concat([merged_df, other_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0001eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_upgrade_time(delay_df, filename='figures/RQ1/overall_upgrade_time.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69de5820",
   "metadata": {},
   "source": [
    "##### Ths size of of the chain of superseded PRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e09f034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6217.000000\n",
       "mean        2.607849\n",
       "std         4.504037\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         1.000000\n",
       "75%         2.000000\n",
       "max       154.000000\n",
       "Name: continuous_superseded_count, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_upgrades_df.loc[all_upgrades_df['label'].isin(['Superseding merged PR', 'Superseding closed PR with external upgrade']), 'continuous_superseded_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a90afe",
   "metadata": {},
   "source": [
    "#### Finding 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6c16b",
   "metadata": {},
   "source": [
    "#### To select random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be49c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_df = (all_upgrades_df[all_upgrades_df['label'].isin(['Superseding closed PR with external upgrade', 'Closed PR with external upgrade'])]\n",
    " .sample(n=362, random_state=41))\n",
    "random_sample_df['url'] = \"https://www.github.com/\" + random_sample_df['repo'].str.cat(random_sample_df['matching_commit_hash'], \"/commit/\")\n",
    "random_sample_df = random_sample_df[['url', 'id', 'title', 'label']]\n",
    "random_sample_df['label'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adee737",
   "metadata": {},
   "source": [
    "#### Stats of the manual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80adf127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_label(x):\n",
    "    if x in ['lock-family', 'lock:family']:\n",
    "        return 'groups:family'\n",
    "    elif x == 'release:prep':\n",
    "        return 'release-prep'\n",
    "    elif x in ['group-dev', 'group:dev']:\n",
    "        return 'groups:dev'\n",
    "    elif x in ['group-prod', 'group:prod']:\n",
    "        return 'groups:prod'\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "def adjust_labels(x):\n",
    "    if x == 'groups':\n",
    "        return \"Grouping dependencies\"\n",
    "    elif x == 'others':\n",
    "        return \"Others\"\n",
    "    elif x == 'bot':\n",
    "        return \"Upgraded by other bots\"\n",
    "    elif x == 'release-prep':\n",
    "        return \"Upgraded in the new release\"\n",
    "    elif x == 'different-version':\n",
    "        return \"Upgraded to a newer version\"\n",
    "    elif x == 'required-source-code-changes':\n",
    "        return \"Required source code changes\"\n",
    "    elif x == 'core-lib-major-upgrade':\n",
    "        return \"Upgraded along a core library\"\n",
    "    elif x == 'bug fixes':\n",
    "        return \"Fix bugs\"\n",
    "    elif x == 'change-lockfile':\n",
    "        return \"Create lockfile\"\n",
    "    elif x == 'operator':\n",
    "        return \"Change operator\"\n",
    "    \n",
    "def generate_rq1_manual_analysis_latex_table(df):\n",
    "    \n",
    "    df['high_label'] = df['high_label'].map(adjust_labels)\n",
    "    \n",
    "    # Compute counts\n",
    "    counts = df[\"high_label\"].value_counts(dropna=False)\n",
    "\n",
    "    # Compute percentages\n",
    "    perc = df[\"high_label\"].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "    # Round and correct to sum to 100%\n",
    "    perc_rounded = perc.round(2)\n",
    "    difference = 100 - perc_rounded.sum()\n",
    "    perc_rounded.iloc[-1] += difference\n",
    "\n",
    "    # Build table with high_label included as a column\n",
    "    table = pd.DataFrame({\n",
    "        \"Change type\": counts.index,\n",
    "        \"\\\\#\": counts.values,\n",
    "        \"\\\\%\": perc_rounded.values\n",
    "    })\n",
    "\n",
    "    # Optional: sort if needed\n",
    "    table = table.sort_values(\"\\\\#\", ascending=False)\n",
    "    \n",
    "    latex_table = table.to_latex(\n",
    "        index=False,   # important: keeps high_label as a column\n",
    "        float_format=\"%.2f\",\n",
    "        caption=\"Different changes that developers make outside Dependabot PRs.\",\n",
    "        label=\"tab:high_label_dist\"\n",
    "    )\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "def extract_repo(x):\n",
    "    res = x.split(\"/\")\n",
    "    return res[-4] + '/' + res[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dd493caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_df = pd.read_csv(\"data/rq1_manual_analysis.csv\")\n",
    "\n",
    "random_sample_df.iloc[:, 3] = random_sample_df.iloc[:, 3].map(rename_label)\n",
    "random_sample_df['high_label'] = random_sample_df['label'].map(lambda x: x.split(':')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd43dd8",
   "metadata": {},
   "source": [
    "### RQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea13ee",
   "metadata": {},
   "source": [
    "#### Finding 1: Clustering project based on their upgrade time into four groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "105df279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrl}\n",
      "\\toprule\n",
      " & Count & Percentage \\\\\n",
      "delay_cluster &  &  \\\\\n",
      "\\midrule\n",
      "0) Within 1 Hour & 70 & 10.04% \\\\\n",
      "1) Within 1 Day & 189 & 27.12% \\\\\n",
      "2) Within 1 Week & 187 & 26.83% \\\\\n",
      "3) Beyond 1 Week & 251 & 36.01% \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Compute the median delay_time per project\n",
    "project_median_delay = all_upgrades_df.groupby('repo')['delay_time'].median().reset_index(name='median_delay_time')\n",
    "# project_median_delay['median_delay_time'] /= 60\n",
    "\n",
    "# 2. Define time thresholds in MINUTES\n",
    "# 1 hour = 60 minutes\n",
    "HOUR_THRESHOLD = 60\n",
    "# 1 day = 24 hours * 60 minutes/hour = 1440 minutes\n",
    "DAY_THRESHOLD = 1440\n",
    "# 1 week = 7 days * 1440 minutes/day = 10080 minutes\n",
    "WEEK_THRESHOLD = 10080\n",
    "\n",
    "# 2. Define the bins, starting at 0 and including the max value\n",
    "min_delay = project_median_delay['median_delay_time'].min()\n",
    "max_delay = project_median_delay['median_delay_time'].max()\n",
    "\n",
    "bins = [\n",
    "    0,\n",
    "    HOUR_THRESHOLD,\n",
    "    DAY_THRESHOLD,\n",
    "    WEEK_THRESHOLD\n",
    "]\n",
    "\n",
    "# Ensure the maximum delay time is included by adding it as the last bin edge\n",
    "if max_delay > WEEK_THRESHOLD:\n",
    "    bins.append(max_delay + 0.001)\n",
    "\n",
    "# Remove duplicates and sort to ensure monotonic bins\n",
    "bins = sorted(list(set(bins)))\n",
    "\n",
    "labels = [\n",
    "    '0) Within 1 Hour',\n",
    "    '1) Within 1 Day',\n",
    "    '2) Within 1 Week',\n",
    "    '3) Beyond 1 Week'\n",
    "]\n",
    "\n",
    "# 3. Create the 'delay_cluster' column\n",
    "project_median_delay['delay_cluster'] = pd.cut(\n",
    "    project_median_delay['median_delay_time'],\n",
    "    bins=bins,\n",
    "    labels=labels[:len(bins)-1],\n",
    "    right=True, # (Exclusive start, inclusive end)\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# 4. Count the results\n",
    "cluster_counts = project_median_delay['delay_cluster'].value_counts().reindex(labels, fill_value=0)\n",
    "\n",
    "# 2. Create the summary table (DataFrame)\n",
    "total_projects = cluster_counts.sum()\n",
    "count_and_percent_df = pd.DataFrame({\n",
    "    'Count': cluster_counts,\n",
    "    'Percentage': (cluster_counts / total_projects) * 100\n",
    "})\n",
    "\n",
    "# 3. Format the percentage column for a clean output\n",
    "count_and_percent_df['Percentage'] = count_and_percent_df['Percentage'].map('{:.2f}%'.format)#.reset_index(drop=-1)\n",
    "\n",
    "print(count_and_percent_df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f017d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delay_cluster\n",
       "3) Beyond 1 Week    251\n",
       "1) Within 1 Day     189\n",
       "2) Within 1 Week    187\n",
       "0) Within 1 Hour     70\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_median_delay['delay_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347ac74",
   "metadata": {},
   "source": [
    "##### How many (percentage) projects have at least 20\\% of the upgrades whose time to upgrade is higher than 1 week?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5d59ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of projects with >= 20% late upgrades: 68.58%\n"
     ]
    }
   ],
   "source": [
    "# 1. Identify upgrades that took more than a week (7 days)\n",
    "all_upgrades_df['is_late'] = all_upgrades_df['delay_days'] > 7\n",
    "\n",
    "# 2. Group by repo to calculate the percentage of late upgrades per project\n",
    "# We calculate the mean of the boolean 'is_late' which gives the fraction (0.0 to 1.0)\n",
    "repo_stats = all_upgrades_df.groupby('repo')['is_late'].mean().reset_index()\n",
    "\n",
    "# 3. Count how many projects have at least 20% (0.20) of their upgrades late\n",
    "late_projects_count = len(repo_stats[repo_stats['is_late'] >= 0.20])\n",
    "\n",
    "# 4. Calculate the percentage of such projects out of the total\n",
    "total_projects = len(repo_stats)\n",
    "percentage = (late_projects_count / total_projects) * 100\n",
    "\n",
    "print(f\"Percentage of projects with >= 20% late upgrades: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ed6b9",
   "metadata": {},
   "source": [
    "#### Finding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8cde1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barplot(data, filename=None):\n",
    "    # --- Custom binning ---\n",
    "    def custom_bin(p):\n",
    "        if p == 0:\n",
    "            return \"0%\"\n",
    "        elif 1 <= p <= 10:\n",
    "            return \"1-10%\"\n",
    "        else:\n",
    "            lower = int(p // 10 * 10)\n",
    "            upper = lower + 10\n",
    "            if lower == 100:\n",
    "                lower, upper = 90, 100\n",
    "            return f\"{lower}-{upper}%\"\n",
    "\n",
    "    binned_labels = data.apply(custom_bin)\n",
    "\n",
    "    # Correct ordered categories\n",
    "    ordered_bins = [\n",
    "        \"0%\", \"1-10%\", \"10-20%\", \"30-40%\", \"40-50%\", \n",
    "        \"50-60%\", \"60-70%\", \"70-80%\", \"80-90%\", \"90-100%\",\n",
    "    ]\n",
    "\n",
    "    percentage_counts = binned_labels.value_counts()\n",
    "\n",
    "    # Prepare data for seaborn\n",
    "    plot_df = pd.DataFrame({\n",
    "        \"range\": percentage_counts.index,\n",
    "        \"count\": percentage_counts.values\n",
    "    })\n",
    "\n",
    "    # Convert range to ordered categorical and sort\n",
    "    plot_df[\"range\"] = pd.Categorical(plot_df[\"range\"], categories=ordered_bins, ordered=True)\n",
    "    plot_df = plot_df.sort_values(\"range\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(5.5, 3.3))\n",
    "    ax = sns.barplot(data=plot_df, x=\"range\", y=\"count\", edgecolor=\"none\", color='grey')\n",
    "\n",
    "    plt.xlabel(\"% range of upgrades made externally\")\n",
    "    plt.ylabel(\"# of projects\")\n",
    "    \n",
    "    # --- The Fix: Set horizontal alignment to 'right' ---\n",
    "    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if filename:\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0.01, dpi=300)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8946a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target labels\n",
    "target_labels = [\n",
    "    \"Closed PR with external upgrade\",\n",
    "    \"Superseding closed PR with external upgrade\"\n",
    "]\n",
    "\n",
    "# Compute the percentage for each repo\n",
    "repo_percentages = (\n",
    "    all_upgrades_df.assign(is_target=all_upgrades_df['label'].isin(target_labels))\n",
    "    .groupby('repo')['is_target']\n",
    "    .mean() * 100  # percentage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ad1dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot(\n",
    "    data=repo_percentages,\n",
    "    filename='figures/RQ2/pctg_external_upgrades.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2d5252d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7862266857962698"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(repo_percentages > 0).sum()/len(repo_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c3e0b",
   "metadata": {},
   "source": [
    "### RQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d8e6d",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c874c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependabot_changes_df = pd.read_csv('data/dependabot_changes.csv')\n",
    "dependabot_changes_df['date'] = pd.to_datetime(dependabot_changes_df['date'], utc=True).dt.tz_convert(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependabot_changes_df['repo'].nunique(),697, len(dependabot_changes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babaf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_per_repo_count = dependabot_changes_df.groupby('repo').size().reset_index(name='changes_count')\n",
    "changed_per_repo_count['changes_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_effect_size(delta):\n",
    "    abs_delta = abs(delta)\n",
    "    if abs_delta < 0.147:\n",
    "        return \"negligible\"\n",
    "    elif abs_delta < 0.33:\n",
    "        return \"small\"\n",
    "    elif abs_delta < 0.474:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "    \n",
    "    \n",
    "def compute_cliffs_for_dependabot_changes(dependabot_changed_df, all_upgrades_df):\n",
    "    # ensure datetime\n",
    "    dependabot_changed_df = dependabot_changed_df.copy()\n",
    "    all_upgrades_df = all_upgrades_df.copy()\n",
    "\n",
    "    dependabot_changed_df = dependabot_changed_df.sort_values(['repo', 'date']).reset_index(drop=True)\n",
    "    results = []\n",
    "    counter = 0\n",
    "    for repo, group in dependabot_changed_df.groupby('repo'):\n",
    "        # sort upgrades\n",
    "        repo_upgrades = (\n",
    "            all_upgrades_df[all_upgrades_df['repo'] == repo]\n",
    "            .sort_values('pr_created_at')\n",
    "        )\n",
    "\n",
    "        changes = group.reset_index(drop=True)\n",
    "        for i, row in changes.iterrows():\n",
    "            change_date = row['date']\n",
    "\n",
    "            # Determine the start of the BEFORE window\n",
    "            if i == 0:\n",
    "                # No earlier change -> before window starts at -inf\n",
    "                previous_change_date = pd.Timestamp.min\n",
    "            else:\n",
    "                previous_change_date = changes.iloc[i - 1]['date']\n",
    "\n",
    "            # Determine the end of the AFTER window\n",
    "            next_date = changes.iloc[i + 1]['date'] if i < len(changes) - 1 else pd.Timestamp.max\n",
    "\n",
    "            # New requirement:\n",
    "            # BEFORE = upgrades between (previous_change_date, change_date)\n",
    "            before_mask = (\n",
    "                (repo_upgrades['pr_created_at'] > previous_change_date) &\n",
    "                (repo_upgrades['pr_created_at'] < change_date)\n",
    "            )\n",
    "\n",
    "            # AFTER stays the same\n",
    "            after_mask = (\n",
    "                (repo_upgrades['pr_created_at'] >= change_date) &\n",
    "                (repo_upgrades['pr_created_at'] < next_date)\n",
    "            )\n",
    "\n",
    "            before_times = repo_upgrades.loc[before_mask, 'delay_days'].dropna().values\n",
    "            after_times = repo_upgrades.loc[after_mask, 'delay_days'].dropna().values\n",
    "\n",
    "            # Default outputs\n",
    "            delta, size, median_before, median_after = np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "            # Compute metrics only if both groups exist\n",
    "            if len(before_times) > 0 and len(after_times) > 0:\n",
    "                delta_tuple = cliffs_delta(before_times, after_times)\n",
    "                delta = delta_tuple[0]\n",
    "                size = interpret_effect_size(delta)\n",
    "                median_before = np.median(before_times)\n",
    "                median_after = np.median(after_times)\n",
    "\n",
    "            results.append({\n",
    "                'repo': repo,\n",
    "                'date': change_date,\n",
    "                'cliffs_delta': delta,\n",
    "                'effect_size': size,\n",
    "                'median_before': median_before,\n",
    "                'median_after': median_after,\n",
    "                'num_upgrade_before': before_mask.sum(), \n",
    "                'num_upgrade_after': after_mask.sum() \n",
    "            })\n",
    "        counter += len(changes)\n",
    "\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    result_out = dependabot_changed_df.merge(metrics_df, on=['repo', 'date'], how='left')\n",
    "    result_out =result_out.drop_duplicates(subset=['repo', 'commit_hash'], keep='last').reset_index(drop=True)\n",
    "\n",
    "    return result_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = compute_cliffs_for_dependabot_changes(dependabot_changes_df, all_upgrades_df)\n",
    "\n",
    "no_upgrade_pctg = 100 * (result_df['median_before'].isna().sum()/len(result_df))\n",
    "print(f\"The percentages of changes that had no upgrade before and after: {no_upgrade_pctg:.2f}, upgrades with median before and after: {result_df['median_before'].notna().sum()}\")\n",
    "result_df.loc[result_df['median_after'].notna(), \"num_upgrade_after\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed740dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 - no_upgrade_pctg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = result_df[result_df['median_before'].notna()]\n",
    "\n",
    "result_count_df = result_df.groupby('repo').size().reset_index(name=\"change_count\")\n",
    "\n",
    "clean_repos = result_count_df.loc[result_count_df['change_count']>=1, 'repo'].unique()\n",
    "\n",
    "result_df = result_df[result_df['repo'].isin(clean_repos)]\n",
    "\n",
    "result_df['commit_url'] = result_df['repo'].str.cat(result_df['commit_hash'], '/commit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38972aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dependabot_effectiveness(result_df):\n",
    "    result_df = result_df.rename(columns={\"effect_size\": \"Effect size\"})\n",
    "    summary = (\n",
    "        result_df\n",
    "        .groupby('Effect size')\n",
    "        .apply(lambda x: pd.Series({\n",
    "            'Fast': (x['cliffs_delta'] > 0).sum(),\n",
    "            'Slow': (x['cliffs_delta'] < 0).sum(),\n",
    "            'Total': len(x),\n",
    "            'median_delta': x['cliffs_delta'].median(),\n",
    "            'median_before': x['median_before'].median(),\n",
    "            'median_after': x['median_after'].median(),\n",
    "            'median_ratio_before_after': (x['median_before'].median() / x['median_after'].median())\n",
    "            if x['median_after'].median() != 0 else np.nan\n",
    "        }), include_groups=False)\n",
    "        .sort_values('Fast')\n",
    "        .reset_index()\n",
    "    )\n",
    "    for col in summary.columns[1:4]:\n",
    "        summary[col] = summary[col].astype(int)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = summarize_dependabot_effectiveness(result_df.copy())\n",
    "latex_table = summary_df.iloc[:, :4].to_latex(\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    column_format=\"lccc\",  # adjust as needed\n",
    "    label=\"tab:paper3-dependabot-actions\",\n",
    "    caption=\"The frequency of developer changes to Dependabot config file that contribute to fast and slow upgrades.\",\n",
    "    position=\"h!\",\n",
    ")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a45d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_df.loc[result_df['effect_size']=='large']),len(result_df['repo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49616df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[result_df['effect_size']=='large', 'repo'].nunique(),result_df['repo'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955e05c",
   "metadata": {},
   "source": [
    "#### Report stats of Dependabot configuration changes associated with fast and slow upgrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d0349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_actions_df = pd.read_excel(\"./data/clean_acc_actions_final.xlsx\")\n",
    "slow_actions_df = pd.read_excel(\"./data/clean_slow_actions_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPENDABOT_OPTION_MAPPING = {\n",
    "    'Adopt Dependabot': {\n",
    "        'high-option': 'Config file'\n",
    "    },\n",
    "    'Remove Dependabot': {\n",
    "        'high-option': 'Config file'\n",
    "    },\n",
    "    'Rename file': {\n",
    "        'high-option': 'Config file'\n",
    "    },\n",
    "    'Other ecosystem': {\n",
    "        'label': 'Others',\n",
    "        'high-option': 'Others'\n",
    "    },\n",
    "    'Others': {\n",
    "        'high-option': 'Others'\n",
    "    },\n",
    "    'Enable production dependency updates': {\n",
    "        'label': 'Others',\n",
    "        'high-option': 'Others'\n",
    "    },\n",
    "    'Remove allow': {\n",
    "        'label': 'Others',\n",
    "        'high-option': 'Others'\n",
    "    },\n",
    "    'Many changes': {\n",
    "        'label': 'Others',\n",
    "        'high-option': 'Others'\n",
    "    },\n",
    "    'Increase interval (e.g., from daily to weekly)': {\n",
    "        'high-option': 'schedule'\n",
    "    },\n",
    "    'Decrease interval': {\n",
    "        'high-option': 'schedule'\n",
    "    },\n",
    "    'Set time': {\n",
    "        'label': 'Add/set time',\n",
    "        'high-option': 'schedule'\n",
    "    },\n",
    "    'Add/set time': {\n",
    "        'high-option': 'schedule'\n",
    "    },\n",
    "    'Add/Set time': {\n",
    "        'label': 'Add/set time',\n",
    "        'high-option': 'schedule'\n",
    "    },\n",
    "    'Increase limit (e.g., from 5 to 20)': {\n",
    "        'high-option': 'open-pull-request-limit'\n",
    "    },\n",
    "    'Decrease limit': {\n",
    "        'high-option': 'open-pull-request-limit'\n",
    "    },\n",
    "    'Fix typos': {\n",
    "        'high-option': 'Style'\n",
    "    },\n",
    "    'Ignore unstable dependencies/version': {\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Unignore dependencies': {\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Ignore a dependency that does not support version updates': {\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Ignore major version upgrades': {\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Ignore dependency updates': {\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Ignoring peer-dependency conflicts': {\n",
    "        'label': 'Ignore peer-dependency conflicts',\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Ignore minor/patch version upgrades': {\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Ignore ESM-related dependencies': {\n",
    "        'high-option': 'ignore'\n",
    "    },\n",
    "    'Add npm ecosystem': {\n",
    "        'label': 'Add/remove updates for npm',\n",
    "        'high-option': 'package-ecoystem'\n",
    "    },\n",
    "    'Remove npm ecosystem': {\n",
    "        'label': 'Add/remove updates for npm',\n",
    "        'high-option': 'package-ecoystem'\n",
    "    },\n",
    "    'Group dev/prod dependencies': {\n",
    "        'high-option': 'groups'\n",
    "    },\n",
    "    'Group matching name dependencies': {\n",
    "        'high-option': 'groups'\n",
    "    },\n",
    "    'Group all dependencies': {\n",
    "        'high-option': 'groups'\n",
    "    },\n",
    "    'Ungroup dev dependencies': {\n",
    "        'high-option': 'groups'\n",
    "    },\n",
    "    'Add commit-message': {\n",
    "        'label': 'Add/update/remove commit-message',\n",
    "        'high-option': 'commit-message'\n",
    "    },\n",
    "    'Update commit-message': {\n",
    "        'label': 'Add/update/remove commit-message',\n",
    "        'high-option': 'commit-message'\n",
    "    },\n",
    "    'Add labels': {\n",
    "        'label': 'Add/update/remove labels',\n",
    "        'high-option': 'labels'\n",
    "    },\n",
    "    'Remove labels': {\n",
    "        'label': 'Add/update/remove labels',\n",
    "        'high-option': 'labels'\n",
    "    },\n",
    "    'Update labels': {\n",
    "        'label': 'Add/update/remove labels',\n",
    "        'high-option': 'labels'\n",
    "    },\n",
    "    'Change reviewer': {\n",
    "        'label': 'Add/update/remove reviewers',\n",
    "        'high-option': 'reviewers'\n",
    "    },\n",
    "    'Modify reviewers': {\n",
    "        'label': 'Add/update/remove reviewers',\n",
    "        'high-option': 'reviewers'\n",
    "    },\n",
    "    'Remove reviewers': {\n",
    "        'label': 'Add/update/remove reviewers',\n",
    "        'high-option': 'reviewers'\n",
    "    },\n",
    "    'Reconfigure reviewers': {\n",
    "        'label': 'Add/update/remove reviewers',\n",
    "        'high-option': 'reviewers'\n",
    "    },\n",
    "    'Add reviewers': {\n",
    "        'label': 'Add/update/remove reviewers',\n",
    "        'high-option': 'reviewers'\n",
    "    },\n",
    "    'Update reviewer': {\n",
    "        'label': 'Add/update/remove reviewers',\n",
    "        'high-option': 'reviewers'\n",
    "    },\n",
    "    'Add target-branch': {\n",
    "        'label': 'Add/update/remove target-branch',\n",
    "        'high-option': 'target-branch'\n",
    "    },\n",
    "    'Remove target-branch': {\n",
    "        'label': 'Add/update/remove target-branch',\n",
    "        'high-option': 'target-branch'\n",
    "    },\n",
    "    'Rename target-branch': {\n",
    "        'label': 'Add/update/remove target-branch',\n",
    "        'high-option': 'target-branch'\n",
    "    },\n",
    "    'Modify target-branch': {\n",
    "        'label': 'Add/update/remove target-branch',\n",
    "        'high-option': 'target-branch'\n",
    "    },\n",
    "    'Rebase strategy': {\n",
    "        'label': 'Add rebase-strategy',\n",
    "        'high-option': 'rebase-strategy'\n",
    "    },\n",
    "    'Increase versioning-strategy': {\n",
    "        'label': 'Increase/widen versioning-strategy',\n",
    "        'high-option': 'versioning-strategy'\n",
    "    },\n",
    "    'Change to increase-if-necessary': {\n",
    "        'label': 'Increase/widen versioning-strategy',\n",
    "        'high-option': 'versioning-strategy'\n",
    "    },\n",
    "    'Increase-if-necessary': {\n",
    "        'label': 'Increase/widen versioning-strategy',\n",
    "        'high-option': 'versioning-strategy'\n",
    "    },\n",
    "    'Increase': {\n",
    "        'label': 'Increase/widen versioning-strategy',\n",
    "        'high-option': 'versioning-strategy'\n",
    "    },\n",
    "    'Widen': {\n",
    "        'label': 'Increase/widen versioning-strategy',\n",
    "        'high-option': 'versioning-strategy'\n",
    "    },\n",
    "    'Add assignees': {\n",
    "        'label': 'Add/update assignees',\n",
    "        'high-option': 'assignees'\n",
    "    },\n",
    "    'Modify assignees': {\n",
    "        'label': 'Add/update assignees',\n",
    "        'high-option': 'assignees'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86211fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label(label, mapping):\n",
    "    \"\"\"Return (option, change_label)\"\"\"\n",
    "    if label in mapping:\n",
    "        option = mapping[label].get(\"high-option\", label)\n",
    "        change = mapping[label].get(\"label\", label)\n",
    "    else:\n",
    "        option = label\n",
    "        change = label\n",
    "    return option, change\n",
    "\n",
    "\n",
    "def count_by_option_and_change(df, mapping):\n",
    "    counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for label in df[\"label\"]:\n",
    "        option, change = map_label(label, mapping)\n",
    "        counts[option][change] += 1\n",
    "    \n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77859903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count fast and slow occurrences\n",
    "fast_counts = count_by_option_and_change(fast_actions_df, DEPENDABOT_OPTION_MAPPING)\n",
    "slow_counts = count_by_option_and_change(slow_actions_df, DEPENDABOT_OPTION_MAPPING)\n",
    "\n",
    "# Collect all options and changes\n",
    "options = sorted(set(fast_counts) | set(slow_counts))\n",
    "\n",
    "rows = []\n",
    "others = None\n",
    "for option in options:\n",
    "    changes = sorted(set(fast_counts[option]) | set(slow_counts[option]))\n",
    "    for change in changes:\n",
    "        item = {\n",
    "            \"Option\": option,\n",
    "            \"Change\": change,\n",
    "            \"Fast\": fast_counts[option].get(change, 0),\n",
    "            \"Slow\": slow_counts[option].get(change, 0),\n",
    "        }\n",
    "        if option == 'Others':\n",
    "            others = item.copy()\n",
    "        else:\n",
    "            rows.append(item)\n",
    "rows.append(others)\n",
    "\n",
    "table_df = pd.DataFrame(rows)\n",
    "\n",
    "table_df[\"Total\"] = table_df[\"Fast\"] + table_df[\"Slow\"]\n",
    "\n",
    "option_totals = (\n",
    "    table_df[table_df[\"Option\"] != \"Others\"]\n",
    "    .groupby(\"Option\")[\"Total\"]\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "ordered_options = list(option_totals.index)\n",
    "\n",
    "if \"Others\" in table_df[\"Option\"].values:\n",
    "    ordered_options.append(\"Others\")\n",
    "    \n",
    "table_df[\"Option\"] = pd.Categorical(\n",
    "    table_df[\"Option\"],\n",
    "    categories=ordered_options,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "table_df = table_df.sort_values(\n",
    "    [\"Option\", \"Total\"],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "table_df = table_df.drop(columns=\"Total\")\n",
    "\n",
    "summary_df = (\n",
    "    table_df\n",
    "    .groupby(\"Option\", as_index=False)[[\"Fast\", \"Slow\"]]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Compute total for sorting\n",
    "summary_df[\"Total\"] = summary_df[\"Fast\"] + summary_df[\"Slow\"]\n",
    "\n",
    "# Sort all except Others\n",
    "sorted_options = (\n",
    "    summary_df[summary_df[\"Option\"] != \"Others\"]\n",
    "    .sort_values(\"Total\", ascending=False)[\"Option\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "if \"Others\" in summary_df[\"Option\"].values:\n",
    "    sorted_options.append(\"Others\")\n",
    "\n",
    "summary_df[\"Option\"] = pd.Categorical(\n",
    "    summary_df[\"Option\"],\n",
    "    categories=sorted_options,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "summary_df = (\n",
    "    summary_df\n",
    "    .sort_values(\"Option\")\n",
    "    .drop(columns=\"Total\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_to_latex(df):\n",
    "    lines = []\n",
    "    lines.append(r\"\\begin{table*}[t!]\")\n",
    "    lines.append(r\"\\centering\")\n",
    "    lines.append(r\"\\footnotesize\")\n",
    "    lines.append(r\"\\caption{Comparison of fast and slow actions that are made to the Dependabot configuration file.}\\label{tab:paper3-slow_fast_actions_count}\")\n",
    "    lines.append(r\"\\begin{tabular}{>{\\arraybackslash}p{3cm} r r}\")\n",
    "    lines.append(r\"\\toprule\")\n",
    "    lines.append(r\"\\textbf{Action} & \\textbf{Fast (\\#)} & \\textbf{Slow (\\#)} \\\\\")\n",
    "    lines.append(r\"\\midrule\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        lines.append(\n",
    "            rf\"{row['Option']} & {row['Fast']} & {row['Slow']} \\\\\"\n",
    "        )\n",
    "\n",
    "    lines.append(r\"\\bottomrule\")\n",
    "    lines.append(r\"\\end{tabular}\")\n",
    "    lines.append(r\"\\end{table*}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def to_latex_with_rowspan(df):\n",
    "    latex = []\n",
    "    latex.append(r\"\\begin{table*}[t!]\")\n",
    "    latex.append(r\"\\centering\")\n",
    "    latex.append(r\"\\footnotesize\")\n",
    "    latex.append(r\"\\caption{Distribution of Dependabot changed options that are associated with fast and slow upgrades.}\")\n",
    "    latex.append(r\"\\label{tab:paper3-manual-analysis-dependabot-config}\")\n",
    "    latex.append(r\"\\begin{tabularx}{\\linewidth}{>{\\arraybackslash}p{2.6cm} X r r}\")\n",
    "    latex.append(r\"\\toprule\")\n",
    "    latex.append(r\"\\textbf{Option} & \\textbf{Change} & \\textbf{Fast (\\#)} & \\textbf{Slow (\\#)} \\\\\")\n",
    "    latex.append(r\"\\midrule\")\n",
    "\n",
    "    for option, group in df.groupby(\"Option\", sort=False):\n",
    "        rowspan = len(group)\n",
    "        first = True\n",
    "        for _, row in group.iterrows():\n",
    "            if first:\n",
    "                latex.append(\n",
    "                    rf\"\\multirow{{{rowspan}}}{{*}}{{\\centering {option}}} & \"\n",
    "                    rf\"{row['Change']} & {row['Fast']} & {row['Slow']} \\\\\"\n",
    "                )\n",
    "                first = False\n",
    "            else:\n",
    "                latex.append(\n",
    "                    rf\" & {row['Change']} & {row['Fast']} & {row['Slow']} \\\\\"\n",
    "                )\n",
    "        latex.append(r\"\\hline\")\n",
    "\n",
    "    latex.append(r\"\\end{tabularx}\")\n",
    "    latex.append(r\"\\end{table*}\")\n",
    "\n",
    "    return \"\\n\".join(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_summary_table = summary_to_latex(summary_df)\n",
    "print(latex_summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = to_latex_with_rowspan(table_df)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544130fe",
   "metadata": {},
   "source": [
    "### RQ4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc1696",
   "metadata": {},
   "source": [
    "#### Finding 1: Upgraded across types of dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c7372b",
   "metadata": {},
   "source": [
    "##### Plot the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "78978c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topicgpt_boxplot(data: pd.DataFrame, log_scale=True, filename=None):\n",
    "    topicgpt_freq_df = data.dropna(subset=['topicgpt_label']).groupby('topicgpt_label').size().reset_index(name='count')\n",
    "\n",
    "    TOP_X_LABELS = topicgpt_freq_df.sort_values('count', ascending=False).iloc[:, 0].values.tolist()\n",
    "\n",
    "    # 2. Filter the DataFrame\n",
    "    df_filtered = data[data['topicgpt_label'].isin(TOP_X_LABELS)].copy()\n",
    "\n",
    "    # 3. Sort the labels by the median delay_time for visualization clarity\n",
    "    median_order = df_filtered.groupby('topicgpt_label')['delay_time'].median().sort_values(ascending=False).index\n",
    "\n",
    "    # Calculate counts for annotation\n",
    "    counts = df_filtered['topicgpt_label'].value_counts().loc[median_order]\n",
    "\n",
    "    # Calculate the overall median of the filtered data\n",
    "    overall_median = df_filtered['delay_hours'].median()\n",
    "\n",
    "    # 4. Generate the boxplot with modifications\n",
    "    fig, ax, = plt.subplots(figsize=(10, 4.5))\n",
    "\n",
    "    # Use a 'fancy' color palette (e.g., 'Spectral')\n",
    "    sns.boxplot(\n",
    "        x='topicgpt_label',\n",
    "        y='delay_hours',\n",
    "        data=df_filtered,\n",
    "        order=median_order,\n",
    "        boxprops=dict(facecolor=(0,0,0,0)),\n",
    "        # palette='Set2', # Fancy color palette\n",
    "        patch_artist=True,\n",
    "        showfliers=False,\n",
    "        showcaps=False,\n",
    "        linewidth=0.8,\n",
    "        # boxprops=dict(alpha=0.6, edgecolor='black', facecolor='white')\n",
    "    )\n",
    "\n",
    "    # Apply log scale to the y-axis (Delay Time)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Add a horizontal line for the overall median\n",
    "    # Remember to use the log-transformed value for 'y' if you use axhline on a log-scaled plot\n",
    "    # The 'delay_time' values are already in the 'days' unit\n",
    "    plt.axhline(\n",
    "        y=overall_median,\n",
    "        color='r',            # Red line\n",
    "        linestyle='--',       # Dashed line\n",
    "        linewidth=1.5,\n",
    "        label=f'Overall median: {overall_median:.2f} hours'\n",
    "    )\n",
    "\n",
    "    if log_scale:\n",
    "        ax.set_yscale(\"symlog\")\n",
    "    ax.set_ylim(-0.1)\n",
    "    hours_yticks = [0, 1, 2, 8, 24 * 1, 24 * 4, 24 * 14, 24 * 60, 24 * 180, 24 * 360]\n",
    "    ax.set_yticks(hours_yticks)\n",
    "    hours_ylabels = []\n",
    "    for h in hours_yticks:\n",
    "        if h < 24:\n",
    "            hours_ylabels.append(f\"{h}h\")\n",
    "        elif h < 24 * 30:\n",
    "            hours_ylabels.append(f\"{h // 24}d\")\n",
    "        else:\n",
    "            hours_ylabels.append(f\"{h // (24 * 30)}mo\")\n",
    "    ax.set_yticklabels(hours_ylabels)\n",
    "\n",
    "    plt.xlabel('Type of dependency')\n",
    "    plt.ylabel('Upgrade time') # Updated unit\n",
    "    # plt.ylabel([f\"{b:g}\" for b in original_breaks])\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "\n",
    "    # Add annotations for the number of instances (counts)\n",
    "    for i, label in enumerate(median_order):\n",
    "        count = counts.loc[label]\n",
    "        \n",
    "        # Place the annotation above the 90th percentile of the data for that group, adjusted for log scale\n",
    "        y_pos = df_filtered[df_filtered['topicgpt_label'] == label]['delay_hours'].quantile(0.80) * 1.65\n",
    "        \n",
    "        # Fallback for placement\n",
    "        if pd.isna(y_pos) or y_pos < df_filtered['delay_hours'].min():\n",
    "            y_pos = df_filtered['delay_hours'].median() * 1.65\n",
    "\n",
    "        plt.text(\n",
    "            x=i,\n",
    "            y=y_pos,\n",
    "            s=f\"{count:,}\",\n",
    "            # color='black',\n",
    "            ha='center',\n",
    "            fontsize=8,\n",
    "            weight='bold'\n",
    "        )\n",
    "\n",
    "\n",
    "    # Adjust layout to prevent labels from being cut off\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "\n",
    "    if filename:\n",
    "        # Save the figure\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0.01, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "76626708",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topicgpt_boxplot(\n",
    "    all_upgrades_df, \n",
    "    filename='figures/RQ4/upgrade_time_per_dep_type.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "38954687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     4396.000000\n",
       "mean       158.701175\n",
       "std        816.804490\n",
       "min          0.000556\n",
       "25%          0.069653\n",
       "50%          2.440556\n",
       "75%         31.364028\n",
       "max      15831.001111\n",
       "Name: delay_hours, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_upgrades_df.loc[(all_upgrades_df['lib_repo']=='babel/babel'), 'delay_hours'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "90870b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicgpt_data_df = pd.read_json(\"./data/output/sample/assignment_corrected.jsonl\", lines=True)\n",
    "topicgpt_data_df.rename(columns={'id': 'lib_repo', 'label': 'topicgpt_label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_upgrades_with_topics_df = all_upgrades_df[all_upgrades_df['lib_repo'].isin(topicgpt_data_df['lib_repo'].values)].copy()\n",
    "# all_upgrades_with_topics_df['delay_time'] = all_upgrades_with_topics_df['delay_time'] / (60 * 24)\n",
    "all_upgrades_with_topics_df = pd.merge(\n",
    "    all_upgrades_with_topics_df,\n",
    "    prs_df[[\"repo_pr_id\", \"auto_merge_allowed\"]],\n",
    "    on=\"repo_pr_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e08f25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicgpt_label\n",
       "Linting                10360\n",
       "Utilities               8666\n",
       "Compiler                6017\n",
       "Testing Tools           4792\n",
       "Type definitions        4665\n",
       "UI Libraries            4510\n",
       "Web Services            3887\n",
       "Bundler                 3287\n",
       "Parser                  2788\n",
       "Web Frameworks          1684\n",
       "CLI Libraries           1455\n",
       "Plugins                 1245\n",
       "Security Tools          1208\n",
       "Versioning              1009\n",
       "Filesystem Tools         964\n",
       "Documentation            930\n",
       "Logging Tools            893\n",
       "WebSocket Libraries      863\n",
       "i8n Libraries            799\n",
       "Data Validation          446\n",
       "Configuration Tools      408\n",
       "Database Tools           398\n",
       "Package management       378\n",
       "Location Services        258\n",
       "Data Serialization       228\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_upgrades_df['topicgpt_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da1011f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_upgrades_df.loc[all_upgrades_df['topicgpt_label']=='Logging', 'delay_hours'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42e38c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topicgpt_label</th>\n",
       "      <th>median_upgrade_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Validation</td>\n",
       "      <td>1.064167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Type definitions</td>\n",
       "      <td>1.650833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Package management</td>\n",
       "      <td>3.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compiler</td>\n",
       "      <td>3.966389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Versioning</td>\n",
       "      <td>4.902222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Plugins</td>\n",
       "      <td>5.605556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Linting</td>\n",
       "      <td>5.928611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>UI Libraries</td>\n",
       "      <td>7.138750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Logging Tools</td>\n",
       "      <td>7.419167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Testing Tools</td>\n",
       "      <td>7.634722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Database Tools</td>\n",
       "      <td>10.506944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Web Services</td>\n",
       "      <td>10.869722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Web Frameworks</td>\n",
       "      <td>11.207222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bundler</td>\n",
       "      <td>11.533889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Configuration Tools</td>\n",
       "      <td>11.981389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Documentation</td>\n",
       "      <td>18.106528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>i8n Libraries</td>\n",
       "      <td>18.356944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Parser</td>\n",
       "      <td>18.452639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Filesystem Tools</td>\n",
       "      <td>19.854583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Utilities</td>\n",
       "      <td>19.901667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Location Services</td>\n",
       "      <td>28.772639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Security Tools</td>\n",
       "      <td>29.167361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WebSocket Libraries</td>\n",
       "      <td>31.621944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLI Libraries</td>\n",
       "      <td>32.136944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Serialization</td>\n",
       "      <td>40.636389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topicgpt_label  median_upgrade_time\n",
       "5       Data Validation             1.064167\n",
       "17     Type definitions             1.650833\n",
       "12   Package management             3.057500\n",
       "2              Compiler             3.966389\n",
       "20           Versioning             4.902222\n",
       "14              Plugins             5.605556\n",
       "9               Linting             5.928611\n",
       "18         UI Libraries             7.138750\n",
       "11        Logging Tools             7.419167\n",
       "16        Testing Tools             7.634722\n",
       "6        Database Tools            10.506944\n",
       "22         Web Services            10.869722\n",
       "21       Web Frameworks            11.207222\n",
       "0               Bundler            11.533889\n",
       "3   Configuration Tools            11.981389\n",
       "7         Documentation            18.106528\n",
       "24        i8n Libraries            18.356944\n",
       "13               Parser            18.452639\n",
       "8      Filesystem Tools            19.854583\n",
       "19            Utilities            19.901667\n",
       "10    Location Services            28.772639\n",
       "15       Security Tools            29.167361\n",
       "23  WebSocket Libraries            31.621944\n",
       "1         CLI Libraries            32.136944\n",
       "4    Data Serialization            40.636389"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_upgrades_df.dropna(subset=[\"topicgpt_label\"]).groupby(['topicgpt_label'])['delay_hours'].median().reset_index(name=\"median_upgrade_time\").sort_values('median_upgrade_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6e06221f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      228.000000\n",
       "mean       553.879136\n",
       "std       1540.387147\n",
       "min          0.006944\n",
       "25%          2.185486\n",
       "50%         40.636389\n",
       "75%        466.309097\n",
       "max      18917.949722\n",
       "Name: delay_hours, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_upgrades_df.loc[all_upgrades_df['topicgpt_label']=='Data Serialization', 'delay_hours'].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccc97f",
   "metadata": {},
   "source": [
    "##### Statical analysis between dev and prod dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a2f136f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.807777777777778, 6.254722222222222, (0.12947093522567513, 'negligible'))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = all_upgrades_df.loc[all_upgrades_df['dep_type'].notna()&(all_upgrades_df['dep_type']==\"prod\"), \"delay_hours\"]\n",
    "v2 = all_upgrades_df.loc[all_upgrades_df['dep_type'].notna()&(all_upgrades_df['dep_type']==\"dev\"), \"delay_hours\"]\n",
    "v1.median(), v2.median(), cliffs_delta(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99288489",
   "metadata": {},
   "source": [
    "##### Draw the latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "b1c450c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_topicgpt_data_df = topicgpt_data_df.drop_duplicates(subset=['topicgpt_label'])[['topicgpt_label', 'responses', 'lib_repo']]\n",
    "unique_topicgpt_data_df.rename(columns={\"topicgpt_label\": \"Label\", \"responses\": \"Definition\", \"lib_repo\": \"Library\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "8935ea26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table}[t!]\n",
      "\\centering\n",
      "\\begin{adjustbox}{width=\\columnwidth}\n",
      "\\begin{tabular}{|l|r|r|}\n",
      "\\hline\n",
      "Label & Definition & Library  \\\\ \\hline\n",
      "\\hline\n",
      "Utilities & [1] Utilities: The description describes a cross-platform wrapper around Node's child-process APIs (spawn/spawnSync), which matches Utilities' scope of general-purpose helpers and cross-platform abstractions. (Supporting quote: \"A cross platform solution to node's spawn and spawnSync\") & moxystudio/node-cross-spawn  \\\\ \\hline\n",
      "HTTP & [1] HTTP: The description describes an HTTP server that serves static files and implements HTTP features like Range requests and conditional GET handling, which are specific to the HTTP protocol (e.g., handling request/response headers and RFC behaviors). (Supporting quote: \"Streaming static file server with Range and conditional-GET support\") & pillarjs/send  \\\\ \\hline\n",
      "Parser & [1] Parser: This is a library whose primary purpose is parsing semantic-version strings for Node (i.e., converting version text into a structured representation), so it fits the Parser category. (Supporting quote: \"The semver parser for node (the one npm uses)\") & npm/node-semver  \\\\ \\hline\n",
      "API & [1] API: Implements the Fetch Web API for Node.js — the description explicitly states it brings the Fetch API to Node.js (\"A light-weight module that brings the Fetch API to Node.js\") & node-fetch/node-fetch  \\\\ \\hline\n",
      "CLI & [1] CLI: The description identifies an \"option parser\" specifically for use with yargs, which directly pertains to parsing command-line arguments and building CLIs. (Supporting quote: \"the mighty option parser used by yargs\") & yargs/yargs-parser  \\\\ \\hline\n",
      "Bundler & [1] Bundler: Matches the description stating the package is a web bundler; it directly aligns with bundling JavaScript and assets for web deployment. (Supporting quote: \"An extremely fast bundler for the web\") & evanw/esbuild  \\\\ \\hline\n",
      "Testing & [1] Testing: The description identifies the project as a test runner for JavaScript, which places it in the Testing category. (Supporting quote: \"Spectacular Test Runner for JavaScript\") & karma-runner/karma  \\\\ \\hline\n",
      "Compiler & [1] Compiler: The description explicitly identifies Babel as a compiler for writing next‑generation JavaScript. (Supporting quote: \"🐠 Babel is a compiler for writing next generation JavaScript.\") & babel/babel  \\\\ \\hline\n",
      "Frontend & [1] Frontend: The description refers to transforming styles using JavaScript plugins, which matches tools for processing and transforming styles for web applications (\"Transforming styles with JS plugins\") & postcss/postcss  \\\\ \\hline\n",
      "Realtime communication & [1] Realtime communication: The description explicitly describes a WebSocket client and server for Node.js, which is a bidirectional, low-latency real-time communication technology. (Supporting quote: \"Simple to use, blazing fast and thoroughly tested WebSocket client and server for Node.js\") & websockets/ws  \\\\ \\hline\n",
      "Serialization & [1] Serialization: Base64 is an encoding/decoding scheme for representing binary data as text, which fits under serialization tools for encoding/decoding data representations. (Supporting quote: \"Base64 implementation for JavaScript\") & dankogai/js-base64  \\\\ \\hline\n",
      "Runtime environment & [1] Runtime environment: \"Standard Library\" most closely aligns with runtime-provided APIs and global objects (i.e., the standard library of a JavaScript runtime) rather than a third-party utility or tooling package. (Supporting quote: \"Standard Library\") & zloirock/core-js  \\\\ \\hline\n",
      "Web framework & [1] Web framework: The description explicitly identifies the project as a Vue component framework, which aligns with \"provides a lightweight foundation for building web applications\" — it's a framework for building UI/components with Vue. (Supporting quote: \"🐉 Vue Component Framework\") & vuetifyjs/vuetify  \\\\ \\hline\n",
      "Filesystem & [1] Filesystem: The description states this provides filesystem bindings for tar-stream, matching the topic for file system access and bindings and integrations with archive/stream libraries. (Supporting quote: \"fs bindings for tar-stream\") & mafintosh/tar-fs  \\\\ \\hline\n",
      "Security & [1] Security: The project is an HTML sanitization tool that cleans user-submitted HTML and preserves only whitelisted elements/attributes, matching the Security category for sanitizing user input to prevent XSS/injection (Supporting quote: \"Clean up user-submitted HTML, preserving whitelisted elements and whitelisted attributes on a per-element basis. Built on htmlparser2 for speed and tolerance\") & apostrophecms/sanitize-html  \\\\ \\hline\n",
      "Internationalization & [1] Internationalization: Timezone support is part of locale/globalization features (date/time handling) which falls under internationalization. (Supporting quote: \"Timezone support for moment.js\") & moment/moment-timezone  \\\\ \\hline\n",
      "Linting & [1] Linting: This tool’s purpose is to run code-style and quality checks (formatters and linters) on files staged for commit, which directly relates to linting workflows. (Supporting quote: \"Run tasks like formatters and linters against staged git files\") & lint-staged/lint-staged  \\\\ \\hline\n",
      "Database & [1] Database: The description identifies the package as the MongoDB client driver for Node.js, which fits the \"Database\" category for libraries/drivers that connect to and interact with databases (Supporting quote: \"The official MongoDB Node.js driver\") & mongodb/node-mongodb-native  \\\\ \\hline\n",
      "Logging & [1] Logging: The Countly SDK collects and records product analytics events from websites/web applications and sends them to a remote analytics service, which aligns with libraries that record application events and diagnostics (i.e., logging/telemetry). (Supporting quote: \"Countly Product Analytics SDK for websites and web applications\") & Countly/countly-sdk-web  \\\\ \\hline\n",
      "Documentation & [1] Documentation: The project generates API documentation specifically for JavaScript code, matching the Documentation topic which covers \"tools for generating and publishing API and developer documentation\" (Supporting quote: \"An API documentation generator for JavaScript.\") & jsdoc/jsdoc  \\\\ \\hline\n",
      "Geolocation & [1] Geolocation: This is an IP-to-location (GeoIP) library for Node.js implementing MaxMind's GeoIP API, so it fits the Geolocation category. (Supporting quote: \"Native NodeJS implementation of MaxMind's GeoIP API -- works in node 0.6.3 and above, ask me about other versions\") & geoip-lite/node-geoip  \\\\ \\hline\n",
      "Configuration & [1] Configuration: The description explicitly states the module is for programmatic interaction with an nginx config file, matching tools for reading/parsing/modifying server configuration files. (Supporting quote: \"NodeJS module for interacting programmatically with an nginx configuration file\") & tmont/nginx-conf  \\\\ \\hline\n",
      "Release management & [1] Release management: Matches tools for automating releases — version bumping and changelog generation using semantic versioning and Conventional Commits (Supporting quote: \"Automate versioning and CHANGELOG generation, with semver.org and conventionalcommits.org\") & conventional-changelog/standard-version  \\\\ \\hline\n",
      "Module loader & [1] Module loader: The description is about analyzing and producing graphs of module imports/dependencies (CommonJS, AMD, ES6), which relates to resolving and managing module imports and their dependency relationships. (Supporting quote: \"Create graphs from your CommonJS, AMD or ES6 module dependencies\") & pahen/madge  \\\\ \\hline\n",
      "Type definitions & [1] Type definitions: The description states the repository provides TypeScript type definitions, matching the \"Type definitions\" topic for declaration files and typings. (Supporting quote: \"The repository for high quality TypeScript type definitions.\") & DefinitelyTyped/DefinitelyTyped  \\\\ \\hline\n",
      "Plugins & [1] Plugins: This is an extension/add-on that expands the assets of an existing icon set (Maki), which fits the \"Plugins\" category for extensions that add or modify functionality of a host project. (Supporting quote: \"🐡 An icon expansion pack for Maki\") & rapideditor/temaki  \\\\ \\hline\n",
      "Validation & [1] Validation: This is a library for verifying data against JSON Schema and JSON Type Definition schemas, matching the Validation category for schema-based data verification. (Supporting quote: \"The fastest JSON schema Validator. Supports JSON Schema draft-04/06/07/2019-09/2020-12 and JSON Type Definition (RFC8927)\") & ajv-validator/ajv  \\\\ \\hline\n",
      "Package management & [1] Package management: Lerna is primarily a tool for managing and publishing multiple packages from a single repository, which aligns with package management functionality. (Supporting quote: \"managing and publishing multiple JavaScript/TypeScript packages from the same repository.\") & lerna/lerna  \\\\ \\hline\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\caption{Example Table Caption}\n",
      "\\label{tab:example}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to LaTeX with vertical lines\n",
    "latex_code = unique_topicgpt_data_df.to_latex(\n",
    "    index=False, \n",
    "    escape=False, \n",
    "    column_format=\"|l|r|r|\",  # vertical lines between columns\n",
    "    header=True\n",
    ")\n",
    "\n",
    "\n",
    "# Add horizontal lines manually\n",
    "latex_code = latex_code.replace(\"\\\\toprule\", \"\\\\hline\")\n",
    "latex_code = latex_code.replace(\"\\\\midrule\", \"\\\\hline\")\n",
    "latex_code = latex_code.replace(\"\\\\bottomrule\", \"\\\\hline\")\n",
    "\n",
    "# Add horizontal lines after each row\n",
    "lines = latex_code.splitlines()\n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    if \"&\" in line and \"\\\\\" in line and not line.strip().startswith(\"\\\\hline\"):\n",
    "        line = line.rstrip(\"\\\\\") + \" \\\\\\\\ \\\\hline\"  # add \\hline after each row\n",
    "    new_lines.append(line)\n",
    "\n",
    "latex_code = \"\\n\".join(new_lines)\n",
    "\n",
    "latex_code = f\"\\\\begin{{adjustbox}}{{width=\\\\columnwidth}}\\n{latex_code}\\n\\\\end{{adjustbox}}\"\n",
    "\n",
    "# Wrap everything in table environment with adjustbox\n",
    "full_table = f\"\"\"\n",
    "\\\\begin{{table}}[t!]\n",
    "\\\\centering\n",
    "{latex_code}\n",
    "\\\\caption{{Example Table Caption}}\n",
    "\\\\label{{tab:example}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "\n",
    "print(full_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95309bc1",
   "metadata": {},
   "source": [
    "### RQ5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5e34e",
   "metadata": {},
   "source": [
    "#### Finding 1: Upgrade time across the change types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c93cf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_commits_upgrades_df = all_upgrades_df.dropna(subset=['commit_label']).copy()\n",
    "labeled_commits_upgrades_df = labeled_commits_upgrades_df.explode(\"commit_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1e0b2671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97.00347222222223, 7.510277777777778)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = labeled_commits_upgrades_df.loc[(labeled_commits_upgrades_df['dependabot_exists']==False)&(labeled_commits_upgrades_df['commit_label']==\"Security\"), 'delay_hours']\n",
    "v2 = labeled_commits_upgrades_df.loc[(labeled_commits_upgrades_df['dependabot_exists']==True)&(labeled_commits_upgrades_df['commit_label']==\"Security\"), 'delay_hours']\n",
    "v1.median(), v2.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d0818269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_commit_changes_labels(\n",
    "    data, x, y, xlab, ylab, filename=None, rotation=0,\n",
    "    fill=None, fill_lab=None, log_scale=True, colors=None\n",
    "):\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    # Draw main boxplot\n",
    "    ax = sns.boxplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=fill,\n",
    "        data=data,\n",
    "        palette=colors,\n",
    "        patch_artist=True,\n",
    "        showfliers=False,\n",
    "        showcaps=False,\n",
    "        linewidth=0.8\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # ADD MEDIAN MARKERS + LABELS\n",
    "    # ---------------------------------------------------------\n",
    "    if fill:\n",
    "        hue_levels = data[fill].unique()\n",
    "        n_hue = len(hue_levels)\n",
    "    else:\n",
    "        hue_levels = [None]\n",
    "        n_hue = 1\n",
    "\n",
    "    x_levels = data[x].unique()\n",
    "    base_positions = np.arange(len(x_levels))\n",
    "\n",
    "    total_width = 0.8\n",
    "    each_width = total_width / n_hue\n",
    "\n",
    "    for xi, xv in enumerate(x_levels):\n",
    "        for hi, hv in enumerate(hue_levels):\n",
    "            if fill:\n",
    "                subset = data[(data[x] == xv) & (data[fill] == hv)]\n",
    "            else:\n",
    "                subset = data[data[x] == xv]\n",
    "\n",
    "            if subset.empty:\n",
    "                continue\n",
    "\n",
    "            median_val = subset[y].median()\n",
    "\n",
    "            # x-position\n",
    "            if fill:\n",
    "                xpos = base_positions[xi] - total_width / \\\n",
    "                    2 + each_width/2 + hi * each_width\n",
    "            else:\n",
    "                xpos = base_positions[xi]\n",
    "\n",
    "            # --- Draw the median marker ---\n",
    "            # ax.scatter(xpos, median_val, marker='D', s=20, color='black', zorder=5)\n",
    "\n",
    "            # --- Format median value label ---\n",
    "            if median_val < 24:\n",
    "                label = f\"{median_val:.2f}h\"\n",
    "            elif median_val < 24 * 30:\n",
    "                label = f\"{median_val / 24:.2f}d\"\n",
    "            else:\n",
    "                label = f\"{median_val // (24*30):.2f}mo\"\n",
    "\n",
    "            # --- Draw label next to marker ---\n",
    "            ax.text(\n",
    "                xpos,\n",
    "                median_val,\n",
    "                label,\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=6,\n",
    "                color=\"black\",\n",
    "                fontweight='bold'\n",
    "            )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # y-scale + ticks\n",
    "    if log_scale:\n",
    "        ax.set_yscale(\"symlog\")\n",
    "\n",
    "    ax.set_ylim(-0.1)\n",
    "    hours_yticks = [1, 2, 8, 48, 24*14, 24*60, 24*180]\n",
    "    ax.set_yticks(hours_yticks)\n",
    "\n",
    "    def format_tick(h):\n",
    "        if h < 24:\n",
    "            return f\"{h}h\"\n",
    "        elif h < 24 * 30:\n",
    "            return f\"{h//24}d\"\n",
    "        else:\n",
    "            return f\"{h//(24*30)}mo\"\n",
    "\n",
    "    ax.set_yticklabels([format_tick(h) for h in hours_yticks], fontsize=9)\n",
    "\n",
    "    # Labels\n",
    "    plt.xlabel(xlab, fontsize=9)\n",
    "    plt.ylabel(ylab, fontsize=9)\n",
    "    plt.xticks(rotation=rotation, ha='right', fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Legend\n",
    "    if fill:\n",
    "        ax.legend(\n",
    "            title=fill_lab,\n",
    "            loc='lower left',\n",
    "            frameon=True,\n",
    "            fontsize=8,\n",
    "            title_fontsize=8,\n",
    "            bbox_to_anchor=(0.33, 1),\n",
    "            ncol=len(data[fill].unique())\n",
    "        )\n",
    "    else:\n",
    "        ax.legend([], [], frameon=False)\n",
    "\n",
    "    if filename:\n",
    "        # Save the figure\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0.01, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "39e24fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05730095701530495, 'negligible')\n",
      "MannwhitneyuResult(statistic=1576376046.5, pvalue=5.218985991248044e-36)\n"
     ]
    }
   ],
   "source": [
    "label = \"Security\"\n",
    "x = 'commit_label'\n",
    "y = 'delay_hours'\n",
    "v1, v2 = labeled_commits_upgrades_df[labeled_commits_upgrades_df[x] == label][y], labeled_commits_upgrades_df[labeled_commits_upgrades_df[x] != label][y]\n",
    "print(cliffs_delta(v1, v2))\n",
    "print(mannwhitneyu(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "381c8fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.823888888888888, 9.405)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.median(), v2.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7e7b256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cliffs_delta_by_label(df, label_col='labels', group_col='is_security', numeric_cols=None):\n",
    "    \"\"\"\n",
    "    Compute Cliff's delta and p-values (Mann-Whitney U) between security and \n",
    "    non-security groups for each label value.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame : summary with columns ['label', 'feature', 'cliffs_delta', 'effect_size', 'p_value']\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "        # Remove group_col from numeric_cols if it happens to be numeric\n",
    "        numeric_cols = [c for c in numeric_cols if c != group_col]\n",
    "\n",
    "    for label_val, sub_df in df.groupby(label_col):\n",
    "        sec_df = sub_df[sub_df[group_col] == \"Yes\"]\n",
    "        nonsec_df = sub_df[sub_df[group_col] == \"No\"]\n",
    "\n",
    "        if len(sec_df) == 0 or len(nonsec_df) == 0:\n",
    "            continue \n",
    "\n",
    "        for col in numeric_cols:\n",
    "            v1, v2 = sec_df[col].dropna(), nonsec_df[col].dropna()\n",
    "            \n",
    "            # Ensure there is enough data to perform the test\n",
    "            if len(v1) == 0 or len(v2) == 0:\n",
    "                continue\n",
    "\n",
    "            # 1. Compute Cliff's Delta (Effect Size)\n",
    "            delta, size = cliffs_delta(v1, v2)\n",
    "            \n",
    "            # 2. Compute Mann-Whitney U Test (Significance)\n",
    "            # We use alternative='two-sided' to match standard p-value reporting\n",
    "            stat, p_val = mannwhitneyu(v1, v2, alternative='two-sided')\n",
    "\n",
    "            results.append({\n",
    "                'label': label_val,\n",
    "                'feature': col,\n",
    "                'cliffs_delta': delta,\n",
    "                'effect_size': size,\n",
    "                'p_value': p_val\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(['feature', 'cliffs_delta'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cd77bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = compute_cliffs_delta_by_label(all_upgrades_df.explode(\"commit_label\"), label_col='commit_label', group_col='is_security', numeric_cols=['delay_time'])\n",
    "result_df = result_df[result_df['label']!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0d2ce110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>feature</th>\n",
       "      <th>cliffs_delta</th>\n",
       "      <th>effect_size</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feature</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.391120</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Refactoring</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.377520</td>\n",
       "      <td>medium</td>\n",
       "      <td>3.730701e-121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Resource</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.375044</td>\n",
       "      <td>medium</td>\n",
       "      <td>2.111549e-257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bug</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.371736</td>\n",
       "      <td>medium</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deprecate</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.358905</td>\n",
       "      <td>medium</td>\n",
       "      <td>9.752997e-258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Test</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.344664</td>\n",
       "      <td>medium</td>\n",
       "      <td>4.965312e-220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Security</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.331750</td>\n",
       "      <td>medium</td>\n",
       "      <td>3.646222e-259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Merge</td>\n",
       "      <td>delay_time</td>\n",
       "      <td>0.304811</td>\n",
       "      <td>small</td>\n",
       "      <td>7.561973e-104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label     feature  cliffs_delta effect_size        p_value\n",
       "3      Feature  delay_time      0.391120      medium   0.000000e+00\n",
       "5  Refactoring  delay_time      0.377520      medium  3.730701e-121\n",
       "6     Resource  delay_time      0.375044      medium  2.111549e-257\n",
       "1          Bug  delay_time      0.371736      medium   0.000000e+00\n",
       "2    Deprecate  delay_time      0.358905      medium  9.752997e-258\n",
       "8         Test  delay_time      0.344664      medium  4.965312e-220\n",
       "7     Security  delay_time      0.331750      medium  3.646222e-259\n",
       "4        Merge  delay_time      0.304811       small  7.561973e-104"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ced91",
   "metadata": {},
   "source": [
    "##### Plot boxplot figure for types of changes in the commits (each commit label will have to sub-boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c674773",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_commit_label_upgrades_df = all_upgrades_df.dropna(subset=[\"commit_label\"]).copy()\n",
    "# clean_commit_label_upgrades_df = clean_commit_label_upgrades_df.explode('commit_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed7bbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Bug, upgrade time (change type included): 9.09, upgrade time (change type not included): 6.61\n",
      "Label: Deprecate, upgrade time (change type included): 10.85, upgrade time (change type not included): 7.25\n",
      "Label: Feature, upgrade time (change type included): 9.03, upgrade time (change type not included): 6.62\n",
      "Label: Refactoring, upgrade time (change type included): 11.16, upgrade time (change type not included): 7.74\n",
      "Label: Security, upgrade time (change type included): 14.82, upgrade time (change type not included): 6.75\n",
      "Label: Test, upgrade time (change type included): 11.57, upgrade time (change type not included): 7.22\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract all unique labels\n",
    "all_labels = sorted(set().union(*clean_commit_label_upgrades_df[\"commit_label\"]))\n",
    "\n",
    "all_labels = [l for l in all_labels if l not in ['Merge', 'Resource']]\n",
    "\n",
    "# 2. Build long-format DataFrame for seaborn\n",
    "rows = []\n",
    "for label in all_labels:\n",
    "    yes_mask = clean_commit_label_upgrades_df[\"commit_label\"].apply(lambda x: label in x)\n",
    "\n",
    "    yes_delays = clean_commit_label_upgrades_df.loc[yes_mask, \"delay_hours\"]\n",
    "    repos = clean_commit_label_upgrades_df.loc[yes_mask, \"repo\"]\n",
    "    ids = clean_commit_label_upgrades_df.loc[yes_mask, \"id\"]\n",
    "    rows.append(pd.DataFrame({\n",
    "        \"label\": label,\n",
    "        \"contains\": \"yes\",\n",
    "        \"delay_hours\": yes_delays,\n",
    "        \"repo\": repos,\n",
    "        \"id\": ids,\n",
    "    }))\n",
    "\n",
    "    no_delays = clean_commit_label_upgrades_df.loc[~yes_mask, \"delay_hours\"]\n",
    "    repos = clean_commit_label_upgrades_df.loc[~yes_mask, \"repo\"]\n",
    "    ids = clean_commit_label_upgrades_df.loc[~yes_mask, \"id\"]\n",
    "    rows.append(pd.DataFrame({\n",
    "        \"label\": label,\n",
    "        \"contains\": \"no\",\n",
    "        \"delay_hours\": no_delays,\n",
    "        \"repo\": repos,\n",
    "        \"id\": ids,\n",
    "    }))\n",
    "    \n",
    "    print(f\"Label: {label}, upgrade time (change type included): {yes_delays.median():.2f}, upgrade time (change type not included): {no_delays.median():.2f}\")\n",
    "\n",
    "plot_df = pd.concat(rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "044694e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_commit_changes_labels(\n",
    "    data=plot_df,\n",
    "    x=\"label\",\n",
    "    y=\"delay_hours\",\n",
    "    xlab=\"Change type\",\n",
    "    ylab=\"Upgrade time\",\n",
    "    fill='contains',\n",
    "    fill_lab=\"Change type included\",\n",
    "    log_scale=True,\n",
    "    rotation=45,\n",
    "    colors=colors,\n",
    "    filename='figures/RQ5/upgrade_time_per_change_type.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60427b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Bug ======\n",
      "MannwhitneyuResult(statistic=470082550.5, pvalue=1.7725085849080147e-30)\n",
      "====== Deprecate ======\n",
      "MannwhitneyuResult(statistic=441593336.5, pvalue=1.5319966718641108e-40)\n",
      "====== Feature ======\n",
      "MannwhitneyuResult(statistic=474453870.0, pvalue=4.69163912635631e-35)\n",
      "====== Refactoring ======\n",
      "MannwhitneyuResult(statistic=248530478.0, pvalue=1.6133698899987976e-18)\n",
      "====== Security ======\n",
      "MannwhitneyuResult(statistic=441570659.0, pvalue=1.4130325834643055e-112)\n",
      "====== Test ======\n",
      "MannwhitneyuResult(statistic=412255006.0, pvalue=1.039458447828762e-53)\n"
     ]
    }
   ],
   "source": [
    "for label in all_labels:\n",
    "    print(f\"====== {label} ======\")\n",
    "    print(mannwhitneyu(\n",
    "        plot_df.loc[(plot_df['label']==label)&(plot_df['contains']=='yes'), 'delay_hours'],\n",
    "        plot_df.loc[(plot_df['label']==label)&(plot_df['contains']=='no'), 'delay_hours']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766ab29",
   "metadata": {},
   "source": [
    "#### Finding 2: Heatmap of change types against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f14b80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_changetype_heatmap(data_df: pd.DataFrame, filename=None):\n",
    "    from itertools import combinations\n",
    "    # 1. Get unique labels\n",
    "    all_labels = sorted(set().union(*data_df[\"commit_label\"]))\n",
    "\n",
    "    # Initialize co-occurrence matrix\n",
    "    cooc = pd.DataFrame(0, index=all_labels, columns=all_labels)\n",
    "\n",
    "    # 2. Count co-occurrences\n",
    "    for labels in data_df[\"commit_label\"]:\n",
    "        labels = list(set(labels))  # ensure no duplicates inside a row\n",
    "        for (a, b) in combinations(labels, 2):\n",
    "            cooc.loc[a, b] += 1\n",
    "            cooc.loc[b, a] += 1\n",
    "        # Also count self-pair (how many times the label appears alone or with others)\n",
    "        for a in labels:\n",
    "            cooc.loc[a, a] += 1\n",
    "\n",
    "    # 3. Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.heatmap(\n",
    "        cooc,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        linewidths=.2,\n",
    "        annot_kws={\"size\": 11}  # smaller font size for cell values\n",
    "    )\n",
    "\n",
    "    # Rotate x-axis labels 45 degrees\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # Make y-axis labels horizontal\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.xlabel(\"Change type\")\n",
    "    plt.ylabel(\"Change type\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        # Save the figure\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0.01, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ce0753bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = clean_commit_label_upgrades_df.copy()\n",
    "# 2. Remove undesired labels\n",
    "data_df[\"commit_label\"] = data_df[\"commit_label\"].apply(lambda labels: [x for x in labels if x not in [\"Merge\", \"Resource\"]])\n",
    "data_df = data_df[data_df[\"commit_label\"].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "53efe677",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_changetype_heatmap(\n",
    "    data_df,\n",
    "    \"figures/RQ5/changetype_heatmap.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800d451",
   "metadata": {},
   "source": [
    "#### Finding 3: The upgrade time across version updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bd983d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_semantic_vers_upgrade_time_boxplot(data, median_order, median_values, log_scale=True, filename=None):\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 2.5))\n",
    "    \n",
    "    sns.boxplot(\n",
    "        data=data,\n",
    "        x='change_type',\n",
    "        y='delay_time_hours',\n",
    "        order=median_order,\n",
    "        showfliers=False,\n",
    "        showcaps=False, #\n",
    "        patch_artist=True,\n",
    "        hue='change_type',\n",
    "        # palette='Set3',\n",
    "        boxprops=dict(facecolor= (0, 0, 0, 0) , edgecolor=(0.5, 0.5, 0.5)),\n",
    "        palette='dark:none',\n",
    "        saturation=0.8, linewidth=1.2, ax=ax,\n",
    "        medianprops={\"linewidth\": 1, \"color\": \"grey\"},\n",
    "    )\n",
    "        \n",
    "    plt.xlabel('Version update')\n",
    "    plt.ylabel('Upgrade time')\n",
    "\n",
    "    if log_scale:\n",
    "        ax.set_yscale(\"symlog\")\n",
    "    ax.set_ylim(-0.1)\n",
    "    hours_yticks = [0, 1, 2, 8, 24 * 2, 24 * 14, 24 * 60]\n",
    "    ax.set_yticks(hours_yticks)\n",
    "    hours_ylabels = []\n",
    "    for h in hours_yticks:\n",
    "        if h < 24:\n",
    "            hours_ylabels.append(f\"{h}h\")\n",
    "        elif h < 24 * 30:\n",
    "            hours_ylabels.append(f\"{h // 24}d\")\n",
    "        else:\n",
    "            hours_ylabels.append(f\"{h // (24 * 30)}mo\")\n",
    "    ax.set_yticklabels(hours_ylabels)\n",
    "\n",
    "    for i, category in enumerate(median_order):\n",
    "        median_val = median_values.loc[category]\n",
    "        y_pos = median_val * 4\n",
    "        ax.text(i, y_pos, f\"{median_val:.2f}h\", ha='center', va='top', fontsize=12, color='black')\n",
    "\n",
    "    ax.grid(False)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        # Save the figure\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0.01, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d44adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Conversion ---\n",
    "# Convert 'delay_time' from minutes to hours, creating a new column for plotting\n",
    "plot_data_df = all_upgrades_df[all_upgrades_df['change_type'].notna()].copy()\n",
    "plot_data_df['delay_time_hours'] = plot_data_df['delay_time'] / 60\n",
    "\n",
    "# --- 2. Sort categories by median delay time and calculate medians ---\n",
    "median_values = plot_data_df.groupby('change_type')['delay_time_hours'].median().sort_values(ascending=False)\n",
    "median_order = median_values.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6ef727e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_semantic_vers_upgrade_time_boxplot(\n",
    "    plot_data_df,\n",
    "    median_order,\n",
    "    median_values,\n",
    "    filename=\"figures/RQ5/upgrade_time_per_version_update.pdf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f8590",
   "metadata": {},
   "source": [
    "#### Finding 4: Heatmap of change type and semantic versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "94d06e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique commit_labels and change_types\n",
    "def plot_changetype_commit_label_heatmap(df, filename=None):\n",
    "    df = df[df[\"change_type\"].notna()]\n",
    "    all_commit_labels = sorted(set().union(*df[\"commit_label\"]))\n",
    "    all_change_types = sorted(df[\"change_type\"].unique())\n",
    "\n",
    "    # Initialize co-occurrence matrix\n",
    "    cooc = pd.DataFrame(0, index=all_commit_labels, columns=all_change_types)\n",
    "\n",
    "    # Count co-occurrences\n",
    "    for idx, row in df.iterrows():\n",
    "        for c_label in row[\"commit_label\"]:\n",
    "            cooc.loc[c_label, row[\"change_type\"]] += 1\n",
    "\n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 3.2))\n",
    "    sns.heatmap(\n",
    "        cooc,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        linewidths=.5,\n",
    "        annot_kws={\"size\": 10}  # smaller font size for cell values\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Semantic version\")\n",
    "    plt.ylabel(\"Change type\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        # Save the figure\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0.01, dpi=300)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d137c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_changetype_commit_label_heatmap(\n",
    "    data_df,\n",
    "    \"figures/RQ5/sematic_version_heatmap.pdf\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
